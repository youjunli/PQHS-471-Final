---
title: "PQHS 471 FINAL"
author: "Youjun Li"
date: "May 04, 2018"
output: 
  pdf_document:
    number_sections: true
documentclass: article
fontsize: 11pt
geometry: margin=1.75in
---
```{r,echo=F}
library(knitr)
options(width=50)
opts_chunk$set(tidy.opts=list(width.cutoff=50),tidy=T)
```

\section{Preparation}

```{r}
library('keras')
library(tidyverse)
library(dummies)
library(randomForest)
library(xgboost)
library(caret)
trn=read.table('ticdata2000.txt', header = F)
tst=read.table('ticeval2000.txt', header = F)
vnm=read.table('varnames.txt', header = F)
colnames(trn)=vnm$V1
colnames(tst)=vnm$V1[1:85]
tst.y=read.table('tictgts2000.txt', header = F)
colnames(tst.y)=vnm$V1[86]
```

The data contains 86 variables including one binary outcome which indicates if households in one post code would buy the insurance policy. Most of the variables are ordered factors except that `MOSTYPE`(Customer Subtype) and `MOSHOOFD`(Customer main type) are just factors. Additionally, `MAANTHUI`(Number of houses) and `MGEMOMV`(Avg size household) are numerical. Since only Random Forest deals with ordered factors, I will code those level variables as ordered factors only for Random Forest. For the rest of methods (SVM, Boosting and Neural Networks), I will leave them as numerical. In fact, after testing by Neural Network, treating them as numerical gives better results than treating them as factors (using one-hot) in terms of loss (code can be found in the R script file).

First of all, we check if there is any missing.
```{r}
anyNA(trn)
anyNA(tst)
```

No missing. Then we want to make sure all variables were loaded as numerical.
```{r}
mean(sapply(trn, is.numeric))
mean(sapply(tst, is.numeric))
```

All variables are currently numerical. We know the outcome is binary, then what is the proportion for $0$?
```{r}
1-mean(trn$CARAVAN)      #train
1-mean(tst.y$CARAVAN)    #test
```

For both training and testing set, the proportion for $0$ is a bit higher than $94\%$, which means even if we make a prediction of all $0$, the accuracy will be at least $0.94$. This means our model needs to be really good so that it can out perform the all-zero guessing. This will be challenging.

\section{Random Forest}
As mentioned above, Random Forest can deal with ordered factors. So we will code the variables that way.
```{r}
trn2=trn
tst2=tst
#convert level variables to ordered factors
trn2[,-c(1,2,3,5,86)]=(lapply(trn2[,-c(1,2,3,5,86)], function(x) factor(x, levels=as.character(sort(unique(x))), ordered = T)))
tst2[,-c(1,2,3,5)]=(lapply(tst2[,-c(1,2,3,5)], function(x) factor(x, levels=as.character(sort(unique(x))), ordered = T)))

#convert the two customer type variables to factors
trn2[,c(1,5)]=lapply(trn2[,c(1,5)], function(x) as.factor(x))
tst2[,c(1,5)]=lapply(tst2[,c(1,5)], function(x) as.factor(x))
```

I would have shown results from repeated cross validation for a grid search for `mtry`, but it took more than $4$ hours to run, so I will just use `tuneRF` to find the optimal `mtry`.
```{r}
set.seed(621)
fit.rf=tuneRF(trn2[,-86], as.factor(trn2$CARAVAN),  ntreeTry = 500, stepFactor = 1.5, doBest = T)
print(fit.rf)
```

The results don't look bad (out of bag error rate $6.56\%$), but considering our goal is to out perform $0.94$, I am not so thrilled. Anyway, let's make prediction for the testing set and look at the confusion matrix.
```{r}
rftest=as.data.frame(cbind(tst2,as.factor(tst.y$CARAVAN)))
yhat.rf=predict(fit.rf, newdata = rftest)
yguess=as.factor(c(rep(0,4000),1))[1:4000]
#confusionMatrix(yguess, rftest$`as.factor(tst.y$CARAVAN)`)
confusionMatrix(data = yhat.rf, rftest$`as.factor(tst.y$CARAVAN)`)
```

Still, we failed to out perform $0.94$ with accuracy $0.9398$.

\section{Boosting}
I will use xgboost for this section. The data wrangling will be different from it was for Random Forest as I will only convert the two categorical variables to factors without ordering, leaving the rest as numerical, and then to one-hot for the two factors.

```{r}
trn3=trn
tst3=tst

trn3$grp=rep(100, nrow(trn3))
tst3$grp=rep(101, nrow(tst3))

df3=rbind(trn3[,-86], tst3)
#names(df3)
sum(sapply(df3,is.numeric))
df3[,c(1,5)]=lapply(df3[,c(1,5)], as.factor)

#one hot
df_dum3=dummy.data.frame(df3)
#colnames(df_dum3)

xtrn3=dplyr::filter(df_dum3, grp==100)
mean(sapply(xtrn3, is.numeric))
xtrn3=as.data.frame(apply(xtrn3, 2, as.numeric))
x_train3=data.matrix(xtrn3[,-ncol(xtrn3)])
dim(x_train3)
#sum(sapply(x_train, is.numeric))
y_train3=data.matrix((trn3$CARAVAN))
colnames(y_train3)=colnames(tst.y)
xtst3=filter(df_dum3, grp==101)
xtst3=as.data.frame(apply(xtst3, 2, as.numeric))
x_test3=data.matrix(xtst3[,-ncol(xtst3)])
dim(x_test3)
y_test3=data.matrix(tst.y)


dtrain=xgb.DMatrix(data = x_train3, label = as.numeric(y_train3))
dtest=xgb.DMatrix(data = x_test3, label = as.numeric(y_test3))
```

Now let's do xgboost by trees and see what test accuracy it will give.
```{r}
watchlist=list(train=dtrain, test=dtest)
xgb_params=list(objective = "binary:logistic")
set.seed(621)
bst=xgb.train(params = xgb_params, data=dtrain,  max.depth=2, eta=1,nthread = 1, nround=10, watchlist=watchlist,eval.metric = "error", eval.metric = "logloss")
#3 rounds are sufficient
predxg=predict(bst, dtest)
predxg=as.numeric(predxg>0.5)
confusionMatrix(predxg, tst.y$CARAVAN)
unique(predxg)
```

We get an accuracy of $0.9395$, still not better than $0.94$, but a little improvement over random forest (even Kappa is better, from $0.0539$ to $0.0195$). 

\section{Support Vector Machine}
The form of the data will be the same as boosting, only with predictors and outcome merged.

```{r}
svm_train=xtrn3
svm_train$grp=as.factor(y_train3)
class(svm_train$grp)
dim(svm_train)
colnames(svm_train)[134]='CARAVAN'
svm_test=xtst3
svm_test$grp=as.factor(y_test3)
dim(svm_test)
class(svm_test$grp)
colnames(svm_test)[134]='CARAVAN'
```

After doing cross validation, the best $c$ we found is $0.01$, and we do predictions for the testing set and look at the confusion matrix.
```{r}
grid.c=expand.grid(C = seq(0.01,10,length.out = 10))
trctrl.svm=trainControl(method = "cv", number = 5)

set.seed(621)
svm_Linear=train(CARAVAN ~., data = svm_train, method = "svmLinear",
                    trControl=trctrl.svm,
                    tuneGrid = grid.c,
                    tuneLength = 10)

svm_Linear

predsvm=predict(svm_Linear, svm_test)
confusionMatrix(predsvm, tst.y$CARAVAN)
unique(predsvm)

```

In fact, the SVM model gives a prediction of all $0$. Well, technically an improvement, but even a person who doesn't know anything about machine learning can make this prediction.

\section{Neural Networks}
For Neural Networks, the form of the data will be the same as boosting. 

```{r}
use_session_with_seed(621)
model = keras_model_sequential() %>%
  layer_dense(units = 100, activation = "relu", input_shape = ncol(x_train3), kernel_regularizer = regularizer_l2(l = 0.01)) %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 267, activation = "relu") %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 1, activation = "sigmoid")   #output
summary(model)
model %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_adam(),
  metrics = c("accuracy")
)
history = model %>% fit(
  x_train3, y_train3,
  epochs = 50, batch_size = 100, verbose = 1,
  validation_split = 0.3
)

model %>% evaluate(x_test3, y_test3, verbose = 0) ## 0.94075 on test set
y_pred1 = model %>% predict_classes(x_test3) ## prediction on test set
table(y_test3, y_pred1) ## test set confusion matrix
```

After several runs, I ended up using three layers, $100$ nodes for the first layer with an $l2$ regularizer, $2* \#ofpredictors -1$ nodes for the second layer, $50$ epochs, batch size of $100$. It gives me the best accuracy ($0.94075$) so far but not from all-zero prediction.



\section{Unsupervised Learning}
\subsection{Kmeans}
For kmeans, I use within group sum of squared error to decide the number of clusters.
```{r}
trn5=trn[,c(6:41)]
#determine how many clusters by within group sum of squared error
wss=(nrow(trn5)-1)*sum(apply(trn5, 2, var))
for (i in 2:40)
{
  wss[i]=sum(kmeans(trn5, centers=i, nstart = 10)$withinss)
}
plot(1:40, wss, type = 'b', xlab="Number of Clusters", ylab="Within SSE")

#choose 10
cluster.km=kmeans(trn5, 10, nstart = 10)
```
No obvious "elbow" point in the scree plot, but it goes relatively flat after $10$, so I choose $10$.

\subsection{Hierarchical Clustering}
I will just use the number of clusters decided by kmeans for this section, and compare which measure agrees with kmeans more.
```{r}
dist5=dist(trn5)
plot(hclust(dist5, method = 'average'), labels=F, main='Average')
plot(hclust(dist5, method = 'complete'), labels=F, main='Complete')

cluster.hc.c=cutree(hclust(dist5,method = 'complete'),10)
cluster.hc.a=cutree(hclust(dist5,method = 'average'),10)
table(cluster.hc.c, cluster.hc.a)
table(cluster.hc.c, cluster.km$cluster)
table(cluster.hc.a, cluster.km$cluster)
```

Looks like "complete" agrees with kmeans more.

\subsection{MDS}

```{r}
knitr::include_graphics('MDSplot.png')
```

I set $k=3$ and plot a 3D plot, whose screenshot is shown here. It's hard to see some obvious pattern out of this.

\section{Summary}
Even though Neural Networks gave the best results in terms of accuracy, I consider myself getting lucky this time. It is very arbitary what values I used for the parameters and I don't know a good way to tune them towards the optimal, if the optimal does exist. But for other methods, we can use approaches like cross validation to do exausted search, even though I could take a while. Hence, unless the data is really large, I would still go with methods like boosting first, rather than Neural Network. 